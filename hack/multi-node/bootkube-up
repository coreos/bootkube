#!/bin/bash
set -euo pipefail

un="$(uname)"
local_os="linux"
if [ ${un} == 'Darwin' ]; then
    local_os="darwin"
fi

# Render assets
if [ ! -d "cluster" ]; then
  ../../_output/bin/${local_os}/bootkube render --asset-dir=cluster --api-servers=https://172.17.4.101:443 --etcd-servers=http://172.17.4.51:2379
  # Add rendered kubeconfig to the node user-data
  cat user-data.sample > cluster/user-data && sed 's/^/      /' cluster/auth/kubeconfig >> cluster/user-data
  cp cluster/user-data{,-worker}
  cp cluster/user-data{,-controller}
  sed -i.bak -e '/--node-labels=master=true/d' cluster/user-data-worker
  for i in http_proxy https_proxy HTTP_PROXY HTTPS_PROXY; do
    if [ -n "${!i:-}" ]; then
        sed -i 's;${'"${i}"'};'"${!i}"';' cluster/user-data{,-worker,-controller}
    else
        sed -i '/${'"${i}"'/d' cluster/user-data{,-worker,-controller}
    fi
  done
fi

# Start the VM
vagrant up
vagrant ssh-config c1 > ssh_config

# Copy locally rendered assets to the server
scp -q -F ssh_config -r cluster core@c1:/home/core/cluster
scp -q -F ssh_config ../../_output/bin/linux/bootkube core@c1:/home/core

# Run bootkube
ssh -q -F ssh_config core@c1 "sudo /home/core/bootkube start --asset-dir=/home/core/cluster --etcd-server=http://172.17.4.51:2379 2>> /home/core/bootkube.log"

echo
echo "Bootstrap complete. Access your kubernetes cluster using:"
echo "kubectl --kubeconfig=cluster/auth/kubeconfig get nodes"
echo
